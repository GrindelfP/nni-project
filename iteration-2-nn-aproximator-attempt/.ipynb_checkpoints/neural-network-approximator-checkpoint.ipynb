{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7735505-af60-4b9f-a5d5-fcc37bdf27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0938182-1d6a-453b-86fc-c9cf82c95b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c00e1a-f2e7-446b-9a33-0a89cb0f400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkApproximator:\n",
    "\n",
    "    \n",
    "    def _generate_weights(self) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "            Generates matrix of weights.\n",
    "            @return: returns matrix of random weights.\n",
    "        \"\"\"\n",
    "\n",
    "        weights = [\n",
    "            np.random.uniform(-1.0, 1.0, (self.input_layer_size + 1, self.hidden_layer_size)),\n",
    "            np.random.uniform(-1.0, 1.0, (self.hidden_layer_size + 1, 1))\n",
    "        ]\n",
    "\n",
    "        return weights\n",
    "\n",
    "    \n",
    "    def _generate_neurons(self) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "            Generates matrix of neurons.\n",
    "            @return: returns matrix of neurons.\n",
    "        \"\"\"\n",
    "\n",
    "        matrix = [\n",
    "            np.zeros(self.input_layer_size + 1),\n",
    "            np.zeros(self.hidden_layer_size + 1),\n",
    "            np.zeros(1)                     \n",
    "        ]\n",
    "        \n",
    "        # Set the last elements of the first and second vectors to 1\n",
    "        matrix[0][-1] = 1\n",
    "        matrix[1][-1] = 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=1, \n",
    "        training_set_size=1000,\n",
    "        k1=1,\n",
    "        k2=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Constructs an instance of NeuralNetworkApproximator.\n",
    "            @param input_size: dimesions count of function to approximate. Default value \n",
    "            @param training_set_size: number of trainig samples, which influenses hidden layer size.\n",
    "            @param k1: tunable parameter, which influenses hidden layer size.\n",
    "            @param k2: tunable parameter, which influenses hidden layer size.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_layer_size = input_size\n",
    "        self.hidden_layer_size = int(np.log10(training_set_size)**(-k1) * (k2 * training_set_size/(input_size + 2)))\n",
    "        self.output_layer_size = 1\n",
    "        self.weights = self._generate_weights()\n",
    "        self.neurons = self._generate_neurons()\n",
    "        \n",
    "\n",
    "    def _sigmoid(self, z) -> None:\n",
    "        \"\"\"\n",
    "            Sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _sigmoid_derivative(self, z) -> None:\n",
    "        \"\"\"\n",
    "            Derivative of the sigmoid function.\n",
    "        \"\"\"\n",
    "        return self._sigmoid(z) * (1 - self._sigmoid(z))\n",
    "\n",
    "    def train(self, data, epochs_num=1000, lm_param=0.00001) -> None:\n",
    "        \"\"\"\n",
    "            Trains the neural network using Levenberg-Marquardt algorithm.\n",
    "            \n",
    "            @param data: List of tuples (x, y) representing training samples.\n",
    "            @param epochs_num: Number of epochs for the training process.\n",
    "            @param lm_param: Initial Levenberg-Marquardt parameter (lambda).\n",
    "        \"\"\"\n",
    "        for epoch in tqdm(range(epochs_num), desc=\"Training\", unit=\"epoch\"):\n",
    "            mse = 0  # Mean squared error for the epoch\n",
    "\n",
    "            for x, y in data:\n",
    "                # Forward propagation (x already includes bias as the last element)\n",
    "                x_with_bias = np.append(x, 1)  # Добавляем bias к x\n",
    "                hidden_input = np.dot(x_with_bias, self.weights[0])\n",
    "                hidden_output = np.append(self._sigmoid(hidden_input), 1)  # hidden_output with bias\n",
    "\n",
    "                final_input = np.dot(hidden_output, self.weights[1])\n",
    "                final_output = self._sigmoid(final_input)\n",
    "\n",
    "                # Compute the error (Mean Squared Error)\n",
    "                error = y - final_output\n",
    "                mse += error**2\n",
    "\n",
    "                # Backpropagation\n",
    "                output_delta = error * self._sigmoid_derivative(final_input)\n",
    "                hidden_delta = output_delta.dot(self.weights[1][:-1].T) * self._sigmoid_derivative(hidden_input)\n",
    "\n",
    "                # Update weights using LM algorithm\n",
    "                self.weights[1] += lm_param * np.outer(hidden_output, output_delta)\n",
    "                self.weights[0] += lm_param * np.outer(x_with_bias, hidden_delta)\n",
    "\n",
    "            mse /= len(data)  # Средняя ошибка для эпохи\n",
    "\n",
    "            # Adjust lambda (LM parameter) adaptively\n",
    "            if mse < 1e-6:\n",
    "                break\n",
    "            elif epoch % 10 == 0:\n",
    "                lm_param *= 0.9\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def predict(self, x) -> None:\n",
    "        \"\"\"\n",
    "            Predicts the output for a given input vector x.\n",
    "            \n",
    "            @param x: Input vector for which to predict the output.\n",
    "            @return: Predicted output value.\n",
    "        \"\"\"\n",
    "        x_with_bias = np.append(x, 1)  # Добавляем bias к x\n",
    "        hidden_input = np.dot(x_with_bias, self.weights[0])\n",
    "        hidden_output = np.append(self._sigmoid(hidden_input), 1)  # hidden_output with bias\n",
    "        final_input = np.dot(hidden_output, self.weights[1])\n",
    "        final_output = self._sigmoid(final_input)\n",
    "        return final_output\n",
    "\n",
    "    def test(self, test_data) -> None:\n",
    "        \"\"\"\n",
    "            Tests the neural network on a given dataset and computes the mean squared error.\n",
    "            \n",
    "            @param test_data: List of tuples (x, y) representing test samples.\n",
    "            @return: Mean squared error of predictions.\n",
    "        \"\"\"\n",
    "        total_error = 0\n",
    "        for x, y in test_data:\n",
    "            prediction = self.predict(x)\n",
    "            error = y - prediction\n",
    "            total_error += error**2\n",
    "            \n",
    "        mse = total_error / len(test_data)\n",
    "        return mse\n",
    "\n",
    "    def download_weights_biases(self) -> tuple[list, list, list, list]:\n",
    "        \"\"\"\n",
    "            Returns a list of weights and biases for each layer in the neural network.\n",
    "            Biases are treated as the last weights for each layer.\n",
    "            \n",
    "            @return: A list containing weights and biases for each layer.\n",
    "        \"\"\"\n",
    "        first_array = self.weights[0][:-1]\n",
    "        second_array = self.weights[0][-1]\n",
    "        third_array = self.weights[1][:-1]\n",
    "        fourth_array = self.weights[1][-1]         \n",
    "        \n",
    "        return first_array, second_array, third_array, fourth_array\n",
    "\n",
    "    def info(self) -> None:\n",
    "        \"\"\"\n",
    "            Displays the architecture of the neural network in a formatted table.\n",
    "        \n",
    "            This method prints a table showing the different layers of the neural network\n",
    "            along with the number of parameters for each layer. The table includes:\n",
    "            - Input Layer: Number of input parameters (excluding the bias neuron).\n",
    "            - Hidden Layer: Number of parameters in the hidden layer (including the bias).\n",
    "            - Output Layer: Number of parameters in the output layer (including the bias).\n",
    "            - Total Parameters Count: The sum of all parameters across the layers.\n",
    "        \n",
    "            @returns: None\n",
    "        \"\"\"\n",
    "        table = PrettyTable()\n",
    "        \n",
    "        # Устанавливаем заголовок таблицы\n",
    "        table.title = \"NEURAL NETWORK APPROXIMATOR\"\n",
    "        table.field_names = [\"Layer\", \"Number of Parameters\"]\n",
    "        \n",
    "        # Входной слой\n",
    "        input_params = self.input_layer_size  # Параметры входного слоя (без учета смещения)\n",
    "        table.add_row([\"Input Layer\", input_params])\n",
    "        \n",
    "        # Скрытый слой\n",
    "        hidden_params = self.hidden_layer_size + 1  # Параметры скрытого слоя (включая смещение)\n",
    "        table.add_row([\"Hidden Layer\", hidden_params])\n",
    "        \n",
    "        # Выходной слой\n",
    "        output_params = self.output_layer_size + 1  # Параметры выходного слоя (включая смещение)\n",
    "        table.add_row([\"Output Layer\", output_params])\n",
    "        \n",
    "        # Общее количество параметров\n",
    "        total_params = input_params + hidden_params + output_params\n",
    "        \n",
    "        print(table)\n",
    "        print(f\"Total Parameters Count: {total_params}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae07a113-63b2-47e4-bef5-6bd21c5e9818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|     NEURAL NETWORK APPROXIMATOR     |\n",
      "+--------------+----------------------+\n",
      "|    Layer     | Number of Parameters |\n",
      "+--------------+----------------------+\n",
      "| Input Layer  |          1           |\n",
      "| Hidden Layer |         4167         |\n",
      "| Output Layer |          2           |\n",
      "+--------------+----------------------+\n",
      "Total Parameters Count: 4170\n"
     ]
    }
   ],
   "source": [
    "nn_approximator = NeuralNetworkApproximator(input_size=1, training_set_size=10000, k2=10)\n",
    "nn_approximator.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb33443f-8b8b-4310-88a1-fd6e676d5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b4c8c37-88ac-4826-9bf4-8908df6fa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.random.uniform(1, 100, 50000).reshape(-1, 1)\n",
    "y_values = np.log(x_values)\n",
    "training_data = [(x, y) for x, y in zip(x_values, y_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a3d7655-e18f-4500-9658-0d7a5a3e95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_values = np.random.uniform(1, 100, 50000).reshape(-1, 1)\n",
    "test_y_values = np.log(x_values)\n",
    "test_data = [(x, y) for x, y in zip(test_x_values, test_y_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca5fe3f-242e-4d3f-807c-2921b2c79f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 10/10 [01:04<00:00,  6.43s/epoch]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7.74234249])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_approximator.train(training_data, epochs_num=10, lm_param=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1ff9967-e44a-4bbd-b163-99bb006dcd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test data: [7.88057887]\n"
     ]
    }
   ],
   "source": [
    "mse = nn_approximator.test(test_data)\n",
    "print(f\"Mean Squared Error on test data: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf033ab6-c264-4029-9581-137381c08fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97402808])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_value = nn_approximator.predict(np.array([1]))\n",
    "predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b569cd-b5b7-4a80-938f-b66f24628f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
